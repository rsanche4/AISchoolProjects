{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d9473c-8eea-4846-98ba-24d52a371e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11377fe4-e85a-4da6-aec1-563b75fce381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1797"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a42c97-ad67-4f4a-b89f-2e817795d10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images[0].shape[0]*digits.images[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad0093dc-b4a2-4000-b85a-fc88c4fec236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[[ 0.  0.  0. 12.  4.  0.  0.  0.]\n",
      " [ 0.  0.  6. 14.  1.  0.  0.  0.]\n",
      " [ 0.  0. 14.  2.  0.  0.  0.  0.]\n",
      " [ 0.  2. 14.  1.  4.  2.  0.  0.]\n",
      " [ 0.  4. 16. 15. 12. 15.  5.  0.]\n",
      " [ 0.  3. 16.  6.  0.  5. 11.  0.]\n",
      " [ 0.  0.  9. 11.  4. 13.  5.  0.]\n",
      " [ 0.  0.  1. 11. 16.  9.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the last digit\n",
    "plt.figure(1, figsize=(3, 3))\n",
    "index = 1007\n",
    "print(digits.target[index])\n",
    "print(digits.images[index])\n",
    "plt.imshow(digits.images[index], cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d90e02-ab4e-4894-a27d-f18ae9d81912",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FIRST ATTEMPT\n",
    "\n",
    "# # for simplicity sake, we are keeping sigmoid as the activation on each layer so no differences there\n",
    "# import tqdm\n",
    "# import math\n",
    "# import copy\n",
    "\n",
    "# class ANN:\n",
    "#     def __init__(self, data, labels):\n",
    "#         self.data = data.reshape(data.shape[0], -1, 1)\n",
    "#         self.labels = labels # labels should be 0 or 1\n",
    "#         self.numofsamples = self.labels.shape[0]\n",
    "\n",
    "#     def printArgs(self):\n",
    "#         print(self.data.shape, self.labels.shape)\n",
    "\n",
    "#     def sigmoid(self, z):\n",
    "#         return 1 / (1 + math.exp(-z))\n",
    "\n",
    "#     def z(self, input_column, weights_column, bias):\n",
    "#         # This is just a helper function that given numpy stuff outputs an actual sum which is important cuz thats the z we feed to our sigmoid or softmax\n",
    "#         return np.dot(input_column.T, weights_column)+bias\n",
    "\n",
    "#     def forward_pass(self, data_input, weightsets, biassets): # make sure your data is vectorized when passed to this\n",
    "#         layer_outputs = []\n",
    "#         for l in range(len(self.numlayers)):\n",
    "#             cur_layer_outputs = []\n",
    "#             for colweight in range(weightsets[l].shape[1]):\n",
    "#                 if l==0:\n",
    "#                     oi = self.sigmoid(self.z(data_input, weightsets[l][:,colweight], biassets[l][colweight]))\n",
    "#                 else:\n",
    "#                     oi = self.sigmoid(self.z(np.array(layer_outputs[l-1]).reshape(-1, 1), weightsets[l][:,colweight], biassets[l][colweight]))\n",
    "#                 cur_layer_outputs.append(oi)\n",
    "#             layer_outputs.append(cur_layer_outputs)\n",
    "#         self.cached_forward_pass = layer_outputs\n",
    "#         return layer_outputs # The last value is just the prob of it being that class. So access like [-1][0]\n",
    "    \n",
    "#     def binary_cross_entropy(self, weightsets, biassets):\n",
    "#         current_sum = 0\n",
    "#         for d in range(self.numofsamples):\n",
    "#             ypred = self.forward_pass(self.data[d], weightsets, biassets)[-1][0]\n",
    "#             ytrue = self.labels[d]\n",
    "#             current_sum += ytrue*np.log(ypred)+(1-ytrue)*np.log(1-ypred)\n",
    "#         return -current_sum/self.numofsamples\n",
    "\n",
    "#     def backward_pass(self, weightsets, biassets, learningrate, h=1e-7):\n",
    "#         # (f(x + h) - f(x)) / h)\n",
    "#         weights_adjusted = copy.deepcopy(weightsets)\n",
    "#         for wset in range(len(weightsets)):\n",
    "#             for row in range(weightsets[wset].shape[0]):\n",
    "#                 for col in range(weightsets[wset].shape[1]):\n",
    "#                     weightsets_copy = copy.deepcopy(weightsets)\n",
    "#                     weightsets_copy[wset][row,col] = weightsets_copy[wset][row,col] + h\n",
    "#                     partial_gradient = (self.binary_cross_entropy(weightsets_copy, biassets) - self.binary_cross_entropy(weightsets, biassets)) / h\n",
    "#                     weights_adjusted[wset][row,col] = weightsets[wset][row,col] - learningrate * partial_gradient\n",
    "\n",
    "#         bias_adjusted = copy.deepcopy(biassets)\n",
    "#         for bset in range(len(biassets)):\n",
    "#             for bias in range(len(biassets[bset])):\n",
    "#                 bias_copy = copy.deepcopy(biassets)\n",
    "#                 bias_copy[bset][bias] = bias_copy[bset][bias] + h\n",
    "#                 partial_gradient = (self.binary_cross_entropy(weightsets, bias_copy) - self.binary_cross_entropy(weightsets, biassets)) / h\n",
    "#                 bias_adjusted[bset][bias] = biassets[bset][bias] - learningrate * partial_gradient                \n",
    "\n",
    "#         return weights_adjusted, bias_adjusted\n",
    "                    \n",
    "    \n",
    "#     # make sure that when passing the numlayers ur first is like the input vectorized, so a pixel canvas is 8x8 so 64 and the last value should be 1 in this binary classifier always\n",
    "#     def train(self, iterations=1000, numlayers=[(64,16), (16, 16), (16, 1)], learningrate=0.001):\n",
    "#         self.numlayers = numlayers\n",
    "#         #print(numerical_derivative(lambda x: x**2, 2))\n",
    "        \n",
    "#         weightsets = []\n",
    "#         biassets = []\n",
    "#         for r,c in numlayers:\n",
    "#             weightsets.append(np.zeros((r,c)))\n",
    "#             biassets.append(np.zeros(c))\n",
    "        \n",
    "#         for i in range(iterations):\n",
    "#             loss = self.binary_cross_entropy(weightsets, biassets)\n",
    "#             print(f\"ITERATION: {i} | LOSS: {loss}\")\n",
    "#             weightsets, biassets = self.backward_pass(weightsets, biassets, learningrate)\n",
    "                \n",
    "#         print(\"Model successfully finished training.\")\n",
    "        \n",
    "# # So this thing is simply a binary neural netowrk, what that means is i am giving it just 1 for the class, 0 if its not\n",
    "# # let us train for 0 digit only\n",
    "\n",
    "# s = ANN(digits.images, np.where(digits.target == 0, 1, 0)) # training just for 0\n",
    "# s.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac475e9-a747-4671-aa32-f27b4a99e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SECOND ATTEMPT\n",
    "\n",
    "# import numpy as np\n",
    "# import math\n",
    "# from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# class ANN:\n",
    "#     def __init__(self, data, labels):\n",
    "#         self.data = data.reshape(data.shape[0], -1, 1)  # (samples, input_size, 1)\n",
    "#         self.labels = labels  # Binary labels (0 or 1)\n",
    "#         self.num_samples = labels.shape[0]\n",
    "\n",
    "#     def sigmoid(self, z):\n",
    "#         return 1 / (1 + np.exp(-z))  # Vectorized sigmoid\n",
    "\n",
    "#     def relu(self, z):\n",
    "#         return np.maximum(0, z)  # Vectorized ReLU\n",
    "\n",
    "#     def z(self, input_data, weights, bias):\n",
    "#         return np.dot(input_data.T, weights) + bias  # Vectorized z-calculation\n",
    "\n",
    "#     def forward_pass(self, input_data, weightsets, biassets):\n",
    "#         layer_outputs = []\n",
    "#         current_input = input_data\n",
    "        \n",
    "#         for l in range(len(weightsets)):\n",
    "#             z_values = np.dot(current_input.T, weightsets[l]) + biassets[l]\n",
    "            \n",
    "#             # Use ReLU for hidden layers, sigmoid for output\n",
    "#             if l == len(weightsets) - 1:\n",
    "#                 output = self.sigmoid(z_values)\n",
    "#             else:\n",
    "#                 output = self.relu(z_values)\n",
    "            \n",
    "#             layer_outputs.append(output)\n",
    "#             current_input = output.reshape(-1, 1)  # Prepare for next layer\n",
    "        \n",
    "#         return layer_outputs  # Last layer output is prediction\n",
    "\n",
    "#     def binary_cross_entropy(self, weightsets, biassets):\n",
    "#         total_loss = 0\n",
    "#         for i in range(self.num_samples):\n",
    "#             y_pred = self.forward_pass(self.data[i], weightsets, biassets)[-1].item()\n",
    "#             y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # Avoid log(0)\n",
    "#             y_true = self.labels[i]\n",
    "#             total_loss += y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)\n",
    "#         return -total_loss / self.num_samples\n",
    "\n",
    "#     def compute_gradient(self, weightsets, biassets, h=1e-5):\n",
    "#         # Initialize gradient storage\n",
    "#         weight_gradients = [np.zeros_like(w) for w in weightsets]\n",
    "#         bias_gradients = [np.zeros_like(b) for b in biassets]\n",
    "        \n",
    "#         original_loss = self.binary_cross_entropy(weightsets, biassets)\n",
    "        \n",
    "#         # Compute weight gradients\n",
    "#         for l in range(len(weightsets)):\n",
    "#             for i in range(weightsets[l].shape[0]):\n",
    "#                 for j in range(weightsets[l].shape[1]):\n",
    "#                     # Perturb weight\n",
    "#                     weightsets[l][i, j] += h\n",
    "#                     loss_plus = self.binary_cross_entropy(weightsets, biassets)\n",
    "#                     weightsets[l][i, j] -= 2*h  # Reset and perturb negatively\n",
    "#                     loss_minus = self.binary_cross_entropy(weightsets, biassets)\n",
    "#                     weightsets[l][i, j] += h  # Reset to original\n",
    "                    \n",
    "#                     # Central difference gradient\n",
    "#                     weight_gradients[l][i, j] = (loss_plus - loss_minus) / (2 * h)\n",
    "        \n",
    "#         # Compute bias gradients (similar to weights)\n",
    "#         for l in range(len(biassets)):\n",
    "#             for j in range(biassets[l].shape[0]):\n",
    "#                 biassets[l][j] += h\n",
    "#                 loss_plus = self.binary_cross_entropy(weightsets, biassets)\n",
    "#                 biassets[l][j] -= 2*h\n",
    "#                 loss_minus = self.binary_cross_entropy(weightsets, biassets)\n",
    "#                 biassets[l][j] += h  # Reset\n",
    "                \n",
    "#                 bias_gradients[l][j] = (loss_plus - loss_minus) / (2 * h)\n",
    "        \n",
    "#         return weight_gradients, bias_gradients\n",
    "\n",
    "#     def train(self, iterations=1000, layer_sizes=[(64, 16), (16, 16), (16, 1)], learning_rate=0.001):\n",
    "#         # Initialize weights with He initialization and biases to 0\n",
    "#         weightsets = [np.random.randn(in_size, out_size) * np.sqrt(2. / in_size) \n",
    "#                      for in_size, out_size in layer_sizes]\n",
    "#         biassets = [np.zeros(out_size) for _, out_size in layer_sizes]\n",
    "        \n",
    "#         for epoch in tqdm(range(iterations)):\n",
    "#             # Compute gradients\n",
    "#             weight_grads, bias_grads = self.compute_gradient(weightsets, biassets)\n",
    "            \n",
    "#             # Update weights and biases\n",
    "#             for l in range(len(weightsets)):\n",
    "#                 weightsets[l] -= learning_rate * weight_grads[l]\n",
    "#                 biassets[l] -= learning_rate * bias_grads[l]\n",
    "            \n",
    "#             # Print loss every 100 epochs\n",
    "#             if epoch % 100 == 0:\n",
    "#                 loss = self.binary_cross_entropy(weightsets, biassets)\n",
    "#                 print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "        \n",
    "#         print(\"Training complete.\")\n",
    "#         self.final_weights = weightsets\n",
    "#         self.final_biases = biassets\n",
    "#         return weightsets, biassets\n",
    "    \n",
    "#     def predict(self, data_to_predict):\n",
    "#         data_to_predict = data_to_predict.reshape(data_to_predict.shape[0], -1)  # Flatten input\n",
    "#         return self.forward_pass(self, data_to_predict, self.final_weights, self.final_biases)[-1][0] # the prob that it will belong to class 1\n",
    "        \n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     from sklearn.datasets import load_digits\n",
    "#     digits = load_digits()\n",
    "#     X = digits.images\n",
    "#     ovrModels = []\n",
    "#     for k in range(10):\n",
    "#         ovrModels.append(ANN(X, np.where(digits.target == k, 1, 0)))\n",
    "\n",
    "#     for model in ovrModels:\n",
    "#         final_weights, final_biases = model.train(iterations=1000, learning_rate=0.01)\n",
    "\n",
    "#     # lets try an example from the dataset\n",
    "#     for modeldigit in range(len(ovrModels)):\n",
    "#         print(f\"Prob of digit {modeldigit}: {ovrModels.predict(X[0])}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7b46f7e-ceb1-43d4-beb8-42fae1f3dc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for digit 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                               | 1/500 [00:08<1:08:22,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.8171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████                                                                           | 26/500 [03:32<1:04:20,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Loss = 0.4200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████                                                                       | 51/500 [06:58<1:01:43,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Loss = 0.3762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▎                                                                    | 76/500 [10:23<58:17,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: Loss = 0.3612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▏                                                               | 101/500 [13:49<55:13,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss = 0.3530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████▏                                                           | 126/500 [17:15<51:26,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125: Loss = 0.3478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████▊                                                          | 136/500 [18:46<50:14,  8.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 129\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining model for digit \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    128\u001b[0m     model \u001b[38;5;241m=\u001b[39m FastANN(X, (y \u001b[38;5;241m==\u001b[39m k)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m))\n\u001b[1;32m--> 129\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     ovr_models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Predict on first sample\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 96\u001b[0m, in \u001b[0;36mFastANN.train\u001b[1;34m(self, iterations, layer_sizes, learning_rate, batch_size, n_workers)\u001b[0m\n\u001b[0;32m     92\u001b[0m biassets \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mzeros(out_size) \u001b[38;5;28;01mfor\u001b[39;00m _, out_size \u001b[38;5;129;01min\u001b[39;00m layer_sizes]\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(iterations)):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Compute gradients (parallelized)\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m     weight_grads, bias_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradient_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweightsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiassets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(weightsets)):\n",
      "Cell \u001b[1;32mIn[7], line 82\u001b[0m, in \u001b[0;36mFastANN.compute_gradient_parallel\u001b[1;34m(self, weightsets, biassets, h, n_workers)\u001b[0m\n\u001b[0;32m     80\u001b[0m             bias_grads[l][j] \u001b[38;5;241m=\u001b[39m grad\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 82\u001b[0m             l, i, j, grad \u001b[38;5;241m=\u001b[39m future[\u001b[38;5;241m0\u001b[39m], future[\u001b[38;5;241m1\u001b[39m], future[\u001b[38;5;241m2\u001b[39m], \u001b[43mfuture\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m             weight_grads[l][i,j] \u001b[38;5;241m=\u001b[39m grad\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weight_grads, bias_grads\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# THIRD ATTEMPT\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor  # Parallelize gradient computation\n",
    "\n",
    "class FastANN:\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data.reshape(data.shape[0], -1)  # Flatten to (samples, input_size)\n",
    "        self.labels = labels  # Binary labels (0 or 1)\n",
    "        self.num_samples = labels.shape[0]\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def forward_pass(self, X_batch, weightsets, biassets):\n",
    "        \"\"\"Vectorized forward pass for a batch of samples\"\"\"\n",
    "        activations = X_batch\n",
    "        for l in range(len(weightsets)):\n",
    "            z = np.dot(activations, weightsets[l]) + biassets[l]\n",
    "            activations = self.relu(z) if l < len(weightsets)-1 else self.sigmoid(z)\n",
    "        return activations\n",
    "    \n",
    "    def binary_cross_entropy(self, weightsets, biassets, batch_size=32):\n",
    "        \"\"\"Mini-batch BCE loss\"\"\"\n",
    "        total_loss = 0\n",
    "        for i in range(0, self.num_samples, batch_size):\n",
    "            X_batch = self.data[i:i+batch_size]\n",
    "            y_batch = self.labels[i:i+batch_size]\n",
    "            y_pred = self.forward_pass(X_batch, weightsets, biassets)\n",
    "            y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n",
    "            batch_loss = -np.mean(y_batch * np.log(y_pred) + (1-y_batch) * np.log(1-y_pred))\n",
    "            total_loss += batch_loss * len(X_batch)\n",
    "        return total_loss / self.num_samples\n",
    "    \n",
    "    def compute_gradient_parallel(self, weightsets, biassets, h=1e-5, n_workers=4):\n",
    "        \"\"\"Parallel gradient computation using central differences\"\"\"\n",
    "        def grad_for_param(l, i, j, is_bias=False):\n",
    "            original = biassets[l][j] if is_bias else weightsets[l][i,j]\n",
    "            \n",
    "            if is_bias:\n",
    "                biassets[l][j] += h\n",
    "                loss_plus = self.binary_cross_entropy(weightsets, biassets)\n",
    "                biassets[l][j] -= 2*h\n",
    "                loss_minus = self.binary_cross_entropy(weightsets, biassets)\n",
    "                biassets[l][j] = original\n",
    "            else:\n",
    "                weightsets[l][i,j] += h\n",
    "                loss_plus = self.binary_cross_entropy(weightsets, biassets)\n",
    "                weightsets[l][i,j] -= 2*h\n",
    "                loss_minus = self.binary_cross_entropy(weightsets, biassets)\n",
    "                weightsets[l][i,j] = original\n",
    "                \n",
    "            return (loss_plus - loss_minus) / (2*h)\n",
    "        \n",
    "        # Compute gradients in parallel\n",
    "        weight_grads = [np.zeros_like(w) for w in weightsets]\n",
    "        bias_grads = [np.zeros_like(b) for b in biassets]\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "            # Weight gradients\n",
    "            futures = []\n",
    "            for l in range(len(weightsets)):\n",
    "                for i in range(weightsets[l].shape[0]):\n",
    "                    for j in range(weightsets[l].shape[1]):\n",
    "                        futures.append((l,i,j, executor.submit(grad_for_param, l, i, j)))\n",
    "            \n",
    "            # Bias gradients\n",
    "            for l in range(len(biassets)):\n",
    "                for j in range(biassets[l].shape[0]):\n",
    "                    futures.append((l,j, 'bias', executor.submit(grad_for_param, l, 0, j, True)))\n",
    "            \n",
    "            # Collect results\n",
    "            for future in futures:\n",
    "                if future[2] == 'bias':\n",
    "                    l, j, _, grad = future[0], future[1], future[2], future[3].result()\n",
    "                    bias_grads[l][j] = grad\n",
    "                else:\n",
    "                    l, i, j, grad = future[0], future[1], future[2], future[3].result()\n",
    "                    weight_grads[l][i,j] = grad\n",
    "                    \n",
    "        return weight_grads, bias_grads\n",
    "    \n",
    "    def train(self, iterations=1000, layer_sizes=[(64,16), (16,16), (16,1)], \n",
    "              learning_rate=0.001, batch_size=32, n_workers=4):\n",
    "        # Initialize weights and biases\n",
    "        weightsets = [np.random.randn(in_size, out_size) * np.sqrt(2./in_size) \n",
    "                     for in_size, out_size in layer_sizes]\n",
    "        biassets = [np.zeros(out_size) for _, out_size in layer_sizes]\n",
    "        \n",
    "        for epoch in tqdm(range(iterations)):\n",
    "            # Compute gradients (parallelized)\n",
    "            weight_grads, bias_grads = self.compute_gradient_parallel(\n",
    "                weightsets, biassets, n_workers=n_workers)\n",
    "            \n",
    "            # Update parameters\n",
    "            for l in range(len(weightsets)):\n",
    "                weightsets[l] -= learning_rate * weight_grads[l]\n",
    "                biassets[l] -= learning_rate * bias_grads[l]\n",
    "            \n",
    "            if epoch % 25 == 0:\n",
    "                loss = self.binary_cross_entropy(weightsets, biassets, batch_size)\n",
    "                print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "        \n",
    "        self.final_weights = weightsets\n",
    "        self.final_biases = biassets\n",
    "        return weightsets, biassets\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Vectorized prediction\"\"\"\n",
    "        X = X.reshape(X.shape[0], -1)  # Flatten input\n",
    "        return self.forward_pass(X, self.final_weights, self.final_biases).flatten()\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.datasets import load_digits\n",
    "    digits = load_digits()\n",
    "    X = digits.images\n",
    "    y = digits.target\n",
    "    \n",
    "    # Train one-vs-rest models\n",
    "    ovr_models = []\n",
    "    for k in range(10):\n",
    "        print(f\"\\nTraining model for digit {k}\")\n",
    "        model = FastANN(X, (y == k).astype(int))\n",
    "        model.train(iterations=500, learning_rate=0.01)\n",
    "        ovr_models.append(model)\n",
    "    \n",
    "    # Predict on first sample\n",
    "    sample = X[0]\n",
    "    probs = [model.predict(sample[np.newaxis])[0] for model in ovr_models]\n",
    "    predicted_digit = np.argmax(probs)\n",
    "    print(f\"\\nPredicted digit: {predicted_digit}\")\n",
    "    print(f\"Class probabilities: {np.round(probs, 4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
